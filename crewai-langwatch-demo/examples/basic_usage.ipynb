{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrewAI + LangWatch Scenarios Demo\n",
    "\n",
    "This notebook demonstrates how to use **LangWatch Scenarios** to test **CrewAI** multi-agent systems through AI-powered simulation testing.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Multi-Agent Systems**: How to build collaborative AI agent teams with CrewAI\n",
    "2. **AI Testing**: Using AI agents to test other AI agents with LangWatch scenarios\n",
    "3. **Realistic Scenarios**: Creating comprehensive test scenarios for complex interactions\n",
    "4. **Quality Evaluation**: Using custom judges to evaluate agent performance\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have:\n",
    "- OpenAI API key set in your environment\n",
    "- All required packages installed (`pip install -r requirements.txt`)\n",
    "- Basic understanding of AI agents and testing concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(project_root / \".env\")\n",
    "\n",
    "# Check API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è Please set your OPENAI_API_KEY in the .env file\")\n",
    "else:\n",
    "    print(\"‚úÖ OpenAI API key found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our demo components\n",
    "from agents.customer_service_crew import CustomerServiceCrew\n",
    "from adapters.crew_adapter import create_crew_adapter\n",
    "from scenarios.judges.custom_judges import (\n",
    "    create_quality_judge, \n",
    "    create_technical_judge, \n",
    "    create_escalation_judge\n",
    ")\n",
    "\n",
    "# Import LangWatch scenarios\n",
    "import scenario\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding CrewAI Multi-Agent System\n",
    "\n",
    "Let's start by exploring the CrewAI customer service system we've built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the customer service crew\n",
    "crew = CustomerServiceCrew()\n",
    "\n",
    "# Explore the crew structure\n",
    "crew_info = crew.get_crew_info()\n",
    "print(\"ü§ñ Customer Service Crew Structure:\")\n",
    "print(json.dumps(crew_info, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the crew with a simple inquiry\n",
    "print(\"üìû Testing Customer Service Crew\")\n",
    "print(\"Customer: I can't log into my account\")\n",
    "print(\"\\nü§ñ Crew Response:\")\n",
    "\n",
    "response = crew.handle_inquiry(\n",
    "    \"I can't log into my account\", \n",
    "    customer_id=\"DEMO_001\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Setting Up LangWatch Scenarios\n",
    "\n",
    "Now let's configure LangWatch scenarios to test our CrewAI system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LangWatch scenarios\n",
    "scenario.configure(\n",
    "    testing_agent=scenario.TestingAgent(\n",
    "        model=os.getenv(\"SIMULATOR_MODEL\", \"openai/gpt-4o-mini\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the CrewAI adapter for LangWatch\n",
    "crew_adapter = create_crew_adapter()\n",
    "\n",
    "print(\"‚úÖ LangWatch scenarios configured\")\n",
    "print(\"‚úÖ CrewAI adapter created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Running Basic Scenarios\n",
    "\n",
    "Let's run some basic scenarios to test our customer service system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic customer service scenario\n",
    "async def run_basic_scenario():\n",
    "    print(\"üß™ Running Basic Customer Service Scenario\")\n",
    "    \n",
    "    result = await scenario.run(\n",
    "        name=\"basic login troubleshooting\",\n",
    "        description=\"\"\"\n",
    "        User is having trouble logging into their account. They're not particularly \n",
    "        tech-savvy but are cooperative and willing to follow instructions. They have \n",
    "        their login credentials ready and access to their email.\n",
    "        \"\"\",\n",
    "        agents=[\n",
    "            crew_adapter,\n",
    "            scenario.UserSimulatorAgent(),\n",
    "            scenario.JudgeAgent(criteria=[\n",
    "                \"Agent should ask relevant troubleshooting questions\",\n",
    "                \"Agent should provide clear, step-by-step instructions\",\n",
    "                \"Agent should be patient and helpful\",\n",
    "                \"Agent should offer multiple solutions if the first doesn't work\"\n",
    "            ])\n",
    "        ],\n",
    "        max_turns=8\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the scenario\n",
    "basic_result = await run_basic_scenario()\n",
    "\n",
    "print(f\"\\nüìä Scenario Result: {'‚úÖ PASSED' if basic_result.success else '‚ùå FAILED'}\")\n",
    "print(f\"üí¨ Messages exchanged: {len(basic_result.messages)}\")\n",
    "print(f\"üìù Feedback: {basic_result.feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the conversation that took place\n",
    "print(\"üîç Conversation Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, message in enumerate(basic_result.messages):\n",
    "    role = message.get('role', 'unknown')\n",
    "    content = message.get('content', '')\n",
    "    \n",
    "    if role == 'user':\n",
    "        print(f\"\\nüë§ Customer: {content}\")\n",
    "    elif role == 'assistant':\n",
    "        print(f\"\\nü§ñ Agent: {content}\")\n",
    "    \n",
    "    if i >= 10:  # Limit output for readability\n",
    "        remaining = len(basic_result.messages) - i - 1\n",
    "        if remaining > 0:\n",
    "            print(f\"\\n... ({remaining} more messages)\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Advanced Scenarios with Custom Judges\n",
    "\n",
    "Now let's use custom judges to evaluate specific aspects of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario with custom quality judge\n",
    "async def run_quality_evaluation_scenario():\n",
    "    print(\"üéØ Running Quality Evaluation Scenario\")\n",
    "    \n",
    "    result = await scenario.run(\n",
    "        name=\"customer service quality evaluation\",\n",
    "        description=\"\"\"\n",
    "        Customer is frustrated about a billing issue that has been ongoing for weeks.\n",
    "        They're not angry but are clearly stressed and need empathetic, professional help.\n",
    "        \"\"\",\n",
    "        agents=[\n",
    "            crew_adapter,\n",
    "            scenario.UserSimulatorAgent(),\n",
    "            create_quality_judge()  # Our custom quality judge\n",
    "        ],\n",
    "        max_turns=10\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "quality_result = await run_quality_evaluation_scenario()\n",
    "\n",
    "print(f\"\\nüìä Quality Evaluation: {'‚úÖ PASSED' if quality_result.success else '‚ùå FAILED'}\")\n",
    "print(f\"üí¨ Messages: {len(quality_result.messages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse and display the quality evaluation results\n",
    "try:\n",
    "    # The custom judge returns JSON evaluation\n",
    "    judge_feedback = quality_result.feedback\n",
    "    \n",
    "    # Try to parse as JSON if it's a string\n",
    "    if isinstance(judge_feedback, str):\n",
    "        try:\n",
    "            evaluation_data = json.loads(judge_feedback)\n",
    "        except json.JSONDecodeError:\n",
    "            evaluation_data = {\"raw_feedback\": judge_feedback}\n",
    "    else:\n",
    "        evaluation_data = judge_feedback\n",
    "    \n",
    "    print(\"üéØ Quality Evaluation Results:\")\n",
    "    print(json.dumps(evaluation_data, indent=2))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not parse evaluation results: {e}\")\n",
    "    print(f\"Raw feedback: {quality_result.feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Technical Support Scenario\n",
    "\n",
    "Let's test how the system handles technical support requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technical support scenario\n",
    "async def run_technical_scenario():\n",
    "    print(\"üîß Running Technical Support Scenario\")\n",
    "    \n",
    "    result = await scenario.run(\n",
    "        name=\"API integration support\",\n",
    "        description=\"\"\"\n",
    "        Developer is trying to integrate the company's API into their application. \n",
    "        They're experiencing authentication issues and getting error codes they don't \n",
    "        understand. They're technically competent but new to this specific API.\n",
    "        \"\"\",\n",
    "        agents=[\n",
    "            crew_adapter,\n",
    "            scenario.UserSimulatorAgent(),\n",
    "            create_technical_judge()  # Our custom technical judge\n",
    "        ],\n",
    "        max_turns=12\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "technical_result = await run_technical_scenario()\n",
    "\n",
    "print(f\"\\nüìä Technical Scenario: {'‚úÖ PASSED' if technical_result.success else '‚ùå FAILED'}\")\n",
    "print(f\"üí¨ Messages: {len(technical_result.messages)}\")\n",
    "print(f\"üìù Technical Evaluation: {technical_result.feedback[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Escalation Scenario\n",
    "\n",
    "Let's test how the system handles escalation situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalation scenario\n",
    "async def run_escalation_scenario():\n",
    "    print(\"üìà Running Escalation Scenario\")\n",
    "    \n",
    "    result = await scenario.run(\n",
    "        name=\"customer escalation handling\",\n",
    "        description=\"\"\"\n",
    "        Customer has been trying to resolve an issue for weeks and is frustrated.\n",
    "        They want to speak to a manager and are considering canceling their service.\n",
    "        The issue is complex and requires escalation to higher-level support.\n",
    "        \"\"\",\n",
    "        agents=[\n",
    "            crew_adapter,\n",
    "            scenario.UserSimulatorAgent(),\n",
    "            create_escalation_judge()  # Our custom escalation judge\n",
    "        ],\n",
    "        max_turns=10\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "escalation_result = await run_escalation_scenario()\n",
    "\n",
    "print(f\"\\nüìä Escalation Scenario: {'‚úÖ PASSED' if escalation_result.success else '‚ùå FAILED'}\")\n",
    "print(f\"üí¨ Messages: {len(escalation_result.messages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display escalation evaluation\n",
    "try:\n",
    "    escalation_feedback = escalation_result.feedback\n",
    "    if isinstance(escalation_feedback, str):\n",
    "        try:\n",
    "            escalation_data = json.loads(escalation_feedback)\n",
    "            print(\"üìà Escalation Evaluation:\")\n",
    "            print(json.dumps(escalation_data, indent=2))\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Escalation feedback: {escalation_feedback}\")\n",
    "    else:\n",
    "        print(f\"Escalation evaluation: {escalation_feedback}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error displaying escalation results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Scripted Scenario Example\n",
    "\n",
    "Let's create a scripted scenario to test specific conversation flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scripted scenario for error recovery\n",
    "async def run_scripted_scenario():\n",
    "    print(\"üìù Running Scripted Error Recovery Scenario\")\n",
    "    \n",
    "    result = await scenario.run(\n",
    "        name=\"scripted error recovery\",\n",
    "        description=\"\"\"\n",
    "        Test how the agent recovers from providing incorrect information.\n",
    "        The script forces the agent to make a mistake, then tests recovery.\n",
    "        \"\"\",\n",
    "        agents=[\n",
    "            crew_adapter,\n",
    "            scenario.UserSimulatorAgent(),\n",
    "            scenario.JudgeAgent(criteria=[\n",
    "                \"Agent should acknowledge the mistake when corrected\",\n",
    "                \"Agent should apologize for the incorrect information\",\n",
    "                \"Agent should provide correct information promptly\",\n",
    "                \"Agent should not make excuses\"\n",
    "            ])\n",
    "        ],\n",
    "        script=[\n",
    "            scenario.user(\"I need help with my billing\"),\n",
    "            scenario.agent(\"I can help with that. I see you have a Premium plan for $99/month.\"),\n",
    "            scenario.user(\"That's not right, I have the Basic plan for $29/month\"),\n",
    "            scenario.proceed()  # Let the scenario continue naturally\n",
    "        ],\n",
    "        max_turns=8\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "scripted_result = await run_scripted_scenario()\n",
    "\n",
    "print(f\"\\nüìä Scripted Scenario: {'‚úÖ PASSED' if scripted_result.success else '‚ùå FAILED'}\")\n",
    "print(f\"üí¨ Messages: {len(scripted_result.messages)}\")\n",
    "print(f\"üìù Feedback: {scripted_result.feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Results Summary and Analysis\n",
    "\n",
    "Let's summarize all our test results and analyze the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "all_results = {\n",
    "    \"basic_scenario\": {\n",
    "        \"success\": basic_result.success,\n",
    "        \"messages\": len(basic_result.messages),\n",
    "        \"feedback\": basic_result.feedback\n",
    "    },\n",
    "    \"quality_evaluation\": {\n",
    "        \"success\": quality_result.success,\n",
    "        \"messages\": len(quality_result.messages),\n",
    "        \"feedback\": quality_result.feedback\n",
    "    },\n",
    "    \"technical_support\": {\n",
    "        \"success\": technical_result.success,\n",
    "        \"messages\": len(technical_result.messages),\n",
    "        \"feedback\": technical_result.feedback\n",
    "    },\n",
    "    \"escalation_handling\": {\n",
    "        \"success\": escalation_result.success,\n",
    "        \"messages\": len(escalation_result.messages),\n",
    "        \"feedback\": escalation_result.feedback\n",
    "    },\n",
    "    \"scripted_scenario\": {\n",
    "        \"success\": scripted_result.success,\n",
    "        \"messages\": len(scripted_result.messages),\n",
    "        \"feedback\": scripted_result.feedback\n",
    "    }\n",
    "}\n",
    "\n",
    "# Calculate summary statistics\n",
    "total_scenarios = len(all_results)\n",
    "passed_scenarios = sum(1 for result in all_results.values() if result[\"success\"])\n",
    "total_messages = sum(result[\"messages\"] for result in all_results.values())\n",
    "avg_messages = total_messages / total_scenarios\n",
    "\n",
    "print(\"üìä Test Results Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total Scenarios: {total_scenarios}\")\n",
    "print(f\"Passed: {passed_scenarios} ‚úÖ\")\n",
    "print(f\"Failed: {total_scenarios - passed_scenarios} ‚ùå\")\n",
    "print(f\"Success Rate: {(passed_scenarios/total_scenarios)*100:.1f}%\")\n",
    "print(f\"Total Messages: {total_messages}\")\n",
    "print(f\"Average Messages per Scenario: {avg_messages:.1f}\")\n",
    "\n",
    "print(\"\\nüìã Individual Results:\")\n",
    "for scenario_name, result in all_results.items():\n",
    "    status = \"‚úÖ\" if result[\"success\"] else \"‚ùå\"\n",
    "    print(f\"  {scenario_name}: {status} ({result['messages']} messages)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "import datetime\n",
    "\n",
    "results_file = project_root / \"results\" / f\"notebook_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "results_file.parent.mkdir(exist_ok=True)\n",
    "\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"üíæ Results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Key Insights and Takeaways\n",
    "\n",
    "Based on our testing, here are the key insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Key Insights from Testing\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "insights = [\n",
    "    \"ü§ñ Multi-Agent Collaboration: CrewAI enables sophisticated agent collaboration\",\n",
    "    \"üß™ AI-Powered Testing: LangWatch scenarios provide realistic testing environments\",\n",
    "    \"üìä Custom Evaluation: Specialized judges can evaluate domain-specific criteria\",\n",
    "    \"üìù Scripted Control: Scripted scenarios allow testing specific conversation flows\",\n",
    "    \"üîÑ Iterative Improvement: Results provide actionable feedback for agent improvement\",\n",
    "    \"‚ö° Scalable Testing: Automated scenarios can test many edge cases efficiently\",\n",
    "    \"üé≠ Realistic Simulation: UserSimulatorAgent creates believable customer interactions\",\n",
    "    \"üìà Quality Metrics: Quantitative evaluation enables systematic improvement\"\n",
    "]\n",
    "\n",
    "for insight in insights:\n",
    "    print(f\"  {insight}\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "next_steps = [\n",
    "    \"Expand scenario coverage to include more edge cases\",\n",
    "    \"Implement continuous testing in CI/CD pipeline\",\n",
    "    \"Create domain-specific judges for your use case\",\n",
    "    \"Add performance and load testing scenarios\",\n",
    "    \"Integrate with monitoring and alerting systems\",\n",
    "    \"Build regression testing suite for agent updates\"\n",
    "]\n",
    "\n",
    "for i, step in enumerate(next_steps, 1):\n",
    "    print(f\"  {i}. {step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "\n",
    "1. **Build Multi-Agent Systems** with CrewAI for complex customer service scenarios\n",
    "2. **Test AI Agents with AI** using LangWatch scenarios for realistic evaluation\n",
    "3. **Create Custom Judges** for domain-specific evaluation criteria\n",
    "4. **Use Scripted Scenarios** for controlled testing of specific flows\n",
    "5. **Analyze Results** to identify areas for improvement\n",
    "\n",
    "The combination of CrewAI and LangWatch scenarios provides a powerful framework for building and testing production-ready AI agent systems. The AI-powered testing approach scales much better than manual testing and can uncover edge cases that might be missed in traditional testing approaches.\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "- [CrewAI Documentation](https://docs.crewai.com/)\n",
    "- [LangWatch Scenarios Documentation](https://scenario.langwatch.ai/)\n",
    "- [AI Agent Testing Best Practices](../docs/best-practices.md)\n",
    "- [Advanced Integration Patterns](../docs/advanced-patterns.md)\n",
    "\n",
    "Happy building! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

